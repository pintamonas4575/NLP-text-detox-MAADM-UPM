{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from googletrans import Translator\n",
    "from transformers import Trainer, TrainingArguments, T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prueba traductor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, do my homework for me, you cheap whore!\n"
     ]
    }
   ],
   "source": [
    "trans = Translator()\n",
    "print(trans.translate('Pues hazme los deberes, furcia barata!').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = pd.read_csv('2_detox_dataset_trans.csv')\n",
    "csv = pd.read_csv('english20k_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(df, train_size=0.75, random_state=42) # random_state=42 for reproducibility\n",
    "\n",
    "train_toxic_texts = list(train_dataframe['toxic_sentence'])\n",
    "train_neutral_texts = list(train_dataframe['neutral_sentence'])\n",
    "\n",
    "val_toxic_texts = list(val_dataframe['toxic_sentence'])\n",
    "val_neutral_texts = list(val_dataframe['neutral_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Detoxificador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target_encodings):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.target_encodings['input_ids'][idx])\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.target_encodings['input_ids'][idx].clone().detach()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_encodings_train = tokenizer(train_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_train = tokenizer(train_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "toxic_encodings_val = tokenizer(val_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_val = tokenizer(val_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = DetoxDataset(toxic_encodings_train, neutral_encodings_train)\n",
    "val_dataset = DetoxDataset(toxic_encodings_val, neutral_encodings_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"no\", # NO validar en cada epoch\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    report_to=[\"none\"], # para no pedir login de 'wandb' y otros\n",
    "    fp16=True, # acelerar entrenaminento \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"flan-t5-detoxificado\")\n",
    "tokenizer.save_pretrained(\"flan-t5-detoxificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRUEBAS DETOX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(text: str):\n",
    "    # Tokenizar la oración tóxica\n",
    "    inputs: dict = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Mover los tensores a la GPU, si está disponible\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(inputs[\"input_ids\"])\n",
    "    texto_neutralizado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return texto_neutralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: shut your angry ass up -> Neutral: shut your angry ass up\n",
      "Original: You are a stupid asshole. -> Neutral: You are stupid asshole.\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"shut your angry ass up\",\n",
    "    \"You are a stupid asshole.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    neutral_sentence = detoxify_sentence(sentence)\n",
    "    print(f\"Original: {sentence} -> Neutral: {neutral_sentence}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BUCLE DE TRADUCCIÓN** (26 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamrat Negara gathered the extremists from top to bottom and gave us two Sabbath prayers and training.\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv('1_detox_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "trans = Translator()\n",
    "# print(trans.translate(df.iloc[400]['toxic_sentence'], dest='en').text)\n",
    "print(trans.translate(\"ታምራት ነገራ ፅንፈኞችን ከላይ እስከታች ሰብስበህ ሁለት ሰንበት ፀበልና ስልጠና ስጥልን\", dest='en').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv(\"1_detox_dataset.csv\"))\n",
    "df_trans = pd.DataFrame(columns=[\"toxic_sentence\", \"neutral_sentence\"])\n",
    "traductor = Translator()\n",
    "\n",
    "for index in range(len(df)):\n",
    "    toxic_translated = traductor.translate(df.iloc[index]['toxic_sentence'], dest='en').text\n",
    "    neutral_translated = traductor.translate(df.iloc[index]['neutral_sentence'], dest='en').text\n",
    "    df_trans.loc[len(df_trans)] = [toxic_translated, neutral_translated]\n",
    "    # break\n",
    "\n",
    "df_trans.to_csv(\"2_detox_dataset_trans.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoclon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

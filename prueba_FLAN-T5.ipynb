{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from googletrans import Translator\n",
    "from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prueba traductor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, do my homework for me, you cheap whore!\n"
     ]
    }
   ],
   "source": [
    "trans = Translator()\n",
    "print(trans.translate('Pues hazme los deberes, furcia barata!').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv('3_joined_dataset.csv'))\n",
    "train_dataframe, val_dataframe = train_test_split(df, train_size=0.8, random_state=42) # random_state=42 for reproducibility\n",
    "\n",
    "train_toxic_texts = list(train_dataframe['toxic_sentence'])\n",
    "train_neutral_texts = list(train_dataframe['neutral_sentence'])\n",
    "\n",
    "val_toxic_texts = list(val_dataframe['toxic_sentence'])\n",
    "val_neutral_texts = list(val_dataframe['neutral_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Detoxificador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.task_specific_params = {\n",
    "    \"neutralization\": {\n",
    "        \"early_stopping\": True,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"max_length\": 100,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"num_beams\": 5,\n",
    "        \"prefix\": \"neutralize: \"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parámetros: 76,961,152\n",
      "Parámetros entrenables: 25,891,328\n",
      "Parámetros congelados: 51,069,824\n"
     ]
    }
   ],
   "source": [
    "# Congelar todos los parámetros del modelo\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar las últimas 3 capas del decoder\n",
    "for param in model.decoder.block[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Mantener la capa de salida (`lm_head`) entrenable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# model.encoder.embed_tokens.weight.requires_grad = True\n",
    "# model.decoder.embed_tokens.weight.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total de parámetros: {total_params:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "print(f\"Parámetros congelados: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target_encodings):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.target_encodings['input_ids'][idx].clone().detach()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_encodings_train = tokenizer(train_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_train = tokenizer(train_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "toxic_encodings_val = tokenizer(val_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_val = tokenizer(val_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = DetoxDataset(toxic_encodings_train, neutral_encodings_train)\n",
    "val_dataset = DetoxDataset(toxic_encodings_val, neutral_encodings_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    # evaluation_strategy=\"no\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=146*5, # cada 5 epochs, teniendo batches de 128\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=25,\n",
    "    report_to=[\"none\"], # para no pedir login de 'wandb' y otros\n",
    "    # fp16=True, # acelera el entrenaminento pero lo empeora  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b230bd55124b998aa9adf84c1aaa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9358, 'grad_norm': 0.0750923678278923, 'learning_rate': 0.00017260273972602742, 'epoch': 3.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50736b7cc6394008afd4f31f3397d9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1498144119977951, 'eval_runtime': 47.092, 'eval_samples_per_second': 99.146, 'eval_steps_per_second': 0.786, 'epoch': 5.0}\n",
      "{'loss': 0.1665, 'grad_norm': 0.06426659226417542, 'learning_rate': 0.0001452054794520548, 'epoch': 6.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc36f5343b424ebbb9b643b692124d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14584316313266754, 'eval_runtime': 47.0596, 'eval_samples_per_second': 99.215, 'eval_steps_per_second': 0.786, 'epoch': 10.0}\n",
      "{'loss': 0.1562, 'grad_norm': 0.1168089210987091, 'learning_rate': 0.0001178082191780822, 'epoch': 10.27}\n",
      "{'loss': 0.1519, 'grad_norm': 0.06362780928611755, 'learning_rate': 9.041095890410958e-05, 'epoch': 13.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19b699849fe4d78828d377622b1f6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14495748281478882, 'eval_runtime': 46.7737, 'eval_samples_per_second': 99.821, 'eval_steps_per_second': 0.791, 'epoch': 15.0}\n",
      "{'loss': 0.1481, 'grad_norm': 0.06557337194681168, 'learning_rate': 6.301369863013699e-05, 'epoch': 17.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7cfa1c53e244e9bdf5219c046d4e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14401212334632874, 'eval_runtime': 46.8799, 'eval_samples_per_second': 99.595, 'eval_steps_per_second': 0.789, 'epoch': 20.0}\n",
      "{'loss': 0.145, 'grad_norm': 0.06218307092785835, 'learning_rate': 3.561643835616438e-05, 'epoch': 20.55}\n",
      "{'loss': 0.1443, 'grad_norm': 0.08183270692825317, 'learning_rate': 8.21917808219178e-06, 'epoch': 23.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2378231cdf54fc3937cae1226b6e369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14377838373184204, 'eval_runtime': 47.1456, 'eval_samples_per_second': 99.034, 'eval_steps_per_second': 0.785, 'epoch': 25.0}\n",
      "{'train_runtime': 1669.5538, 'train_samples_per_second': 279.611, 'train_steps_per_second': 2.186, 'train_loss': 0.39603337849656195, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3650, training_loss=0.39603337849656195, metrics={'train_runtime': 1669.5538, 'train_samples_per_second': 279.611, 'train_steps_per_second': 2.186, 'train_loss': 0.39603337849656195, 'epoch': 25.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"checkpoints/checkpoint-3000\") # da error por archivos faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDAR Y CARGAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./modelo_local\\\\tokenizer_config.json',\n",
       " './modelo_local\\\\special_tokens_map.json',\n",
       " './modelo_local\\\\spiece.model',\n",
       " './modelo_local\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./modelo_local\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(save_directory)\n",
    "tokenizer = T5Tokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRUEBAS DETOX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(text: str):\n",
    "    # Tokenizar la oración tóxica\n",
    "    inputs: dict = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Mover los tensores a la GPU, si está disponible\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=100)\n",
    "    texto_neutralizado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return texto_neutralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Do my homework, cheap whore! -> Neutral: Do my homework, cheap whore!\n",
      "Original: shut your angry ass up -> Neutral: Stop talking.\n",
      "Original: You are a stupid person -> Neutral: You are not smart\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"Do my homework, cheap whore!\",\n",
    "    \"shut your angry ass up\",\n",
    "    \"You are a stupid person\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    neutral_sentence = detoxify_sentence(sentence)\n",
    "    print(f\"Original: {sentence} -> Neutral: {neutral_sentence}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BUCLE DE TRADUCCIÓN** (26 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamrat Negara gathered the extremists from top to bottom and gave us two Sabbath prayers and training.\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv('1_detox_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "trans = Translator()\n",
    "# print(trans.translate(df.iloc[400]['toxic_sentence'], dest='en').text)\n",
    "print(trans.translate(\"ታምራት ነገራ ፅንፈኞችን ከላይ እስከታች ሰብስበህ ሁለት ሰንበት ፀበልና ስልጠና ስጥልን\", dest='en').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv(\"1_detox_dataset.csv\"))\n",
    "df_trans = pd.DataFrame(columns=[\"toxic_sentence\", \"neutral_sentence\"])\n",
    "traductor = Translator()\n",
    "\n",
    "for index in range(len(df)):\n",
    "    toxic_translated = traductor.translate(df.iloc[index]['toxic_sentence'], dest='en').text\n",
    "    neutral_translated = traductor.translate(df.iloc[index]['neutral_sentence'], dest='en').text\n",
    "    df_trans.loc[len(df_trans)] = [toxic_translated, neutral_translated]\n",
    "    break\n",
    "\n",
    "df_trans.to_csv(\"2_detox_dataset_trans.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoclon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from googletrans import Translator\n",
    "from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prueba traductor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, do my homework for me, you cheap whore!\n"
     ]
    }
   ],
   "source": [
    "trans = Translator()\n",
    "print(trans.translate('Pues hazme los deberes, furcia barata!').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv('3_joined_dataset.csv'))\n",
    "train_dataframe, val_dataframe = train_test_split(df, train_size=0.75, random_state=42) # random_state=42 for reproducibility\n",
    "\n",
    "train_toxic_texts = list(train_dataframe['toxic_sentence'])\n",
    "train_neutral_texts = list(train_dataframe['neutral_sentence'])\n",
    "\n",
    "val_toxic_texts = list(val_dataframe['toxic_sentence'])\n",
    "val_neutral_texts = list(val_dataframe['neutral_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Detoxificador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parámetros: 76,961,152\n",
      "Parámetros entrenables: 25,891,328\n",
      "Parámetros congelados: 51,069,824\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Congelar todos los parámetros del modelo\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar las últimas 3 capas del decoder\n",
    "for param in model.decoder.block[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Mantener la capa de salida (`lm_head`) entrenable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total de parámetros: {total_params:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "print(f\"Parámetros congelados: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target_encodings):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.target_encodings['input_ids'][idx])\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.target_encodings['input_ids'][idx].clone().detach()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_encodings_train = tokenizer(train_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_train = tokenizer(train_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "toxic_encodings_val = tokenizer(val_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_val = tokenizer(val_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = DetoxDataset(toxic_encodings_train, neutral_encodings_train)\n",
    "val_dataset = DetoxDataset(toxic_encodings_val, neutral_encodings_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    evaluation_strategy=\"no\", # NO validar en cada epoch\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    report_to=[\"none\"], # para no pedir login de 'wandb' y otros\n",
    "    fp16=True, # acelerar entrenaminento \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60ceccbf3a14708b433148687aaad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 3.65}\n",
      "{'train_runtime': 173.9283, 'train_samples_per_second': 503.253, 'train_steps_per_second': 3.938, 'train_loss': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=685, training_loss=0.0, metrics={'train_runtime': 173.9283, 'train_samples_per_second': 503.253, 'train_steps_per_second': 3.938, 'train_loss': 0.0, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDAR Y CARGAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./modelo_local\\\\tokenizer_config.json',\n",
       " './modelo_local\\\\special_tokens_map.json',\n",
       " './modelo_local\\\\spiece.model',\n",
       " './modelo_local\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./modelo_local\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(save_directory)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRUEBAS DETOX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(text: str):\n",
    "    # Tokenizar la oración tóxica\n",
    "    inputs: dict = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Mover los tensores a la GPU, si está disponible\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(inputs[\"input_ids\"])\n",
    "    texto_neutralizado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return texto_neutralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Well, do my homework, you cheap whore. -> Neutral: i'm not a student\n",
      "Original: shut your angry ass up -> Neutral: i'm gonna be a bit shit up\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"Well, do my homework, you cheap whore.\",\n",
    "    \"shut your angry ass up\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    neutral_sentence = detoxify_sentence(sentence)\n",
    "    print(f\"Original: {sentence} -> Neutral: {neutral_sentence}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BUCLE DE TRADUCCIÓN** (26 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamrat Negara gathered the extremists from top to bottom and gave us two Sabbath prayers and training.\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv('1_detox_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "trans = Translator()\n",
    "# print(trans.translate(df.iloc[400]['toxic_sentence'], dest='en').text)\n",
    "print(trans.translate(\"ታምራት ነገራ ፅንፈኞችን ከላይ እስከታች ሰብስበህ ሁለት ሰንበት ፀበልና ስልጠና ስጥልን\", dest='en').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv(\"1_detox_dataset.csv\"))\n",
    "df_trans = pd.DataFrame(columns=[\"toxic_sentence\", \"neutral_sentence\"])\n",
    "traductor = Translator()\n",
    "\n",
    "for index in range(len(df)):\n",
    "    toxic_translated = traductor.translate(df.iloc[index]['toxic_sentence'], dest='en').text\n",
    "    neutral_translated = traductor.translate(df.iloc[index]['neutral_sentence'], dest='en').text\n",
    "    df_trans.loc[len(df_trans)] = [toxic_translated, neutral_translated]\n",
    "    # break\n",
    "\n",
    "df_trans.to_csv(\"2_detox_dataset_trans.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoclon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

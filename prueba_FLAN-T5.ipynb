{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from googletrans import Translator\n",
    "from transformers import Trainer, TrainingArguments, T5TokenizerFast, T5ForConditionalGeneration, EncoderDecoderModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Traductor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, do my homework for me, you cheap whore!\n"
     ]
    }
   ],
   "source": [
    "trans = Translator()\n",
    "print(trans.translate('Pues hazme los deberes, furcia barata!').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('2_detox_dataset_trans.csv')\n",
    "csv = pd.read_csv('english20k_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(df, train_size=0.75, random_state=42) # random_state=42 for reproducibility\n",
    "\n",
    "train_toxic_texts = list(train_dataframe['toxic_sentence'])\n",
    "train_neutral_texts = list(train_dataframe['neutral_sentence'])\n",
    "\n",
    "val_toxic_texts = list(val_dataframe['toxic_sentence'])\n",
    "val_neutral_texts = list(val_dataframe['neutral_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Detoxificador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_encodings_train = tokenizer(train_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_train = tokenizer(train_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "toxic_encodings_val = tokenizer(val_toxic_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "neutral_encodings_val = tokenizer(val_neutral_texts, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target_encodings):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.target_encodings['input_ids'][idx])\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.target_encodings['input_ids'][idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "train_dataset = DetoxDataset(toxic_encodings_train, neutral_encodings_train)\n",
    "val_dataset = DetoxDataset(toxic_encodings_val, neutral_encodings_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    report_to=[\"none\"], # para no pedir login de 'wandb' y otros\n",
    "    fp16=True, # acelerar entrenaminento \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c30ff46fff483fbf278984de974d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ad2087c91a4f34b98d997e6fb90c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.3892, 'eval_samples_per_second': 925.039, 'eval_steps_per_second': 30.835, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee891fb4c504009a23b54d8e5d3bad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.3588, 'eval_samples_per_second': 1003.317, 'eval_steps_per_second': 33.444, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 2.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa6d55791fb48389680ca52451e1291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.361, 'eval_samples_per_second': 997.165, 'eval_steps_per_second': 33.239, 'epoch': 3.0}\n",
      "{'train_runtime': 100.5674, 'train_samples_per_second': 96.652, 'train_steps_per_second': 6.056, 'train_loss': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=609, training_loss=0.0, metrics={'train_runtime': 100.5674, 'train_samples_per_second': 96.652, 'train_steps_per_second': 6.056, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"flan-t5-detoxificado\")\n",
    "tokenizer.save_pretrained(\"flan-t5-detoxificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRUEBAS DETOX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detoxed text: shut your angry ass up\n"
     ]
    }
   ],
   "source": [
    "# texto_prueba = \"Well, do my homework for me, you cheap whore!\" \n",
    "texto_prueba = \"shut your angry ass up\" \n",
    "\n",
    "# Tokenizar la oración tóxica\n",
    "inputs = tokenizer(texto_prueba, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "# Mover los tensores a la GPU, si está disponible\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"])\n",
    "texto_neutralizado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Detoxed text: {texto_neutralizado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(text: str):\n",
    "    # Tokenizar la oración tóxica\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Mover los tensores a la GPU, si está disponible\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(inputs[\"input_ids\"])\n",
    "    texto_neutralizado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return texto_neutralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: shut your angry ass up -> Neutral: shut your angry ass up\n",
      "Original: You are a stupid asshole. -> Neutral: You are stupid asshole.\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"shut your angry ass up\",\n",
    "    \"You are a stupid asshole.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    neutral_sentence = detoxify_sentence(sentence)\n",
    "    print(f\"Original: {sentence} -> Neutral: {neutral_sentence}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BUCLE DE TRADUCCIÓN** (26 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('detox_dataset.csv')\n",
    "df = pd.DataFrame(csv)\n",
    "\n",
    "trans = Translator()\n",
    "# print(trans.translate(df.iloc[400]['toxic_sentence'], dest='en').text)\n",
    "print(trans.translate(\"ታምራት ነገራ ፅንፈኞችን ከላይ እስከታች ሰብስበህ ሁለት ሰንበት ፀበልና ስልጠና ስጥልን\", dest='en').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_trans = pd.read_csv('detox_dataset_trans.csv')\n",
    "df_trans = pd.DataFrame(csv_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df)):\n",
    "    traductor = Translator()\n",
    "    toxic_translated = traductor.translate(df.iloc[index]['toxic_sentence'], dest='en').text\n",
    "    neutral_translated = traductor.translate(df.iloc[index]['neutral_sentence'], dest='en').text\n",
    "    new_row = [toxic_translated, neutral_translated]\n",
    "    df_trans.loc[len(df_trans)] = new_row\n",
    "    # break\n",
    "\n",
    "df_trans.to_csv('detox_dataset_trans.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoclon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

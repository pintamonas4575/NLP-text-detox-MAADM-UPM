{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaime/Escritorio/AIR/entornos/NLP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-30 13:27:30.849246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730291250.878058  290658 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730291250.884332  290658 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 13:27:30.911207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer,AutoModelForSeq2SeqLM,BertTokenizerFast\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import regex as re\n",
    "from googletrans import Translator\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate all phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate with Google Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests:\n",
    "#splits = {'uk': 'data/uk-00000-of-00001-86a756a5ad8b3340.parquet', 'hi': 'data/hi-00000-of-00001-35f39f96bd50e4e4.parquet', 'zh': 'data/zh-00000-of-00001-21350981874ac4f4.parquet', 'ar': 'data/ar-00000-of-00001-2338057ce0eeaf0d.parquet', 'de': 'data/de-00000-of-00001-00ccec8d5e81dbf1.parquet', 'en': 'data/en-00000-of-00001-c54cbb5edf21dd23.parquet', 'ru': 'data/ru-00000-of-00001-6818d57069197d03.parquet', 'am': 'data/am-00000-of-00001-656e19a6f1cc2383.parquet', 'es': 'data/es-00000-of-00001-723005ac428f80c9.parquet'}\n",
    "#df = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox_test/\" + splits[\"es\"])\n",
    "splits = {'eng_Latn': 'data/en-00000-of-00001.parquet', \n",
    "          'rus_Cyrl': 'data/ru-00000-of-00001.parquet', \n",
    "          'ukr_Cyrl': 'data/uk-00000-of-00001.parquet', \n",
    "          'deu_Latn': 'data/de-00000-of-00001.parquet', \n",
    "          'spa_Latn': 'data/es-00000-of-00001.parquet', \n",
    "          'amh_Ethi': 'data/am-00000-of-00001.parquet', \n",
    "          'zho_Hans': 'data/zh-00000-of-00001.parquet', \n",
    "          'arb_Arab': 'data/ar-00000-of-00001.parquet', \n",
    "          'hin_Deva': 'data/hi-00000-of-00001.parquet'}\n",
    "translator = Translator()\n",
    "all_data=[]\n",
    "for key, value in splits.items():\n",
    "    x=0\n",
    "    final_dataframe = pd.DataFrame(columns=['toxic_sentence_not_translated','neutral_sentence_not_translated','toxic_sentence_translated','neutral_sentence_translated','original_language'])\n",
    "    df = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + value)\n",
    "    array = np.array(df)\n",
    "    for instance in array:\n",
    "        input=[]\n",
    "        input.append(instance[0])\n",
    "        input.append(instance[1])\n",
    "        toxic_translated = translator.translate(instance[0]).text\n",
    "        input.append(toxic_translated)\n",
    "        non_toxic_translated = translator.translate(instance[1]).text\n",
    "        input.append(non_toxic_translated)\n",
    "        input.append(key)\n",
    "        final_dataframe.loc[x]=input\n",
    "        x+=1\n",
    "    all_data.append(final_dataframe)\n",
    "big_dataframe=pd.concat(all_data)\n",
    "big_dataframe.to_csv('Dataset_tab.csv',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate with NLLB-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con emojis\n",
    "class Translator_emojis():\n",
    "    def __init__(self):\n",
    "        # Use the original NLLB tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "    def translate(self, src_lan, tgt_lan, text):\n",
    "        # Split text into words and emojis to handle emojis separately\n",
    "        words = re.split(r'(\\p{So}|\\p{Cn})', text, flags=re.UNICODE)\n",
    "        translated_text = []\n",
    "        for word in words:\n",
    "            if word and not re.search(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF]', word):\n",
    "                # Only translate text segments, skip emojis\n",
    "                text_with_lang_code = f\"{src_lan} {word}\"\n",
    "                inputs = self.tokenizer(text_with_lang_code, return_tensors=\"pt\")\n",
    "                translated_tokens = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lan)\n",
    "                )\n",
    "                translated_word = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                # If it's an emoji, keep it as is\n",
    "                print(u'word')\n",
    "                translated_text += u'\\U0001f604'\n",
    "\n",
    "            # Append translated word or emoji to result\n",
    "            translated_text.append(translated_word)\n",
    "        \n",
    "        return translated_text\n",
    "    \n",
    "#Omitiendo emojis\n",
    "class Translator():\n",
    "    def __init__(self):\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.model=AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "    def translate(self,src_lan,tgt_lan,text):\n",
    "        text=f\"{src_lan} {text}\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(** inputs, forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lan))\n",
    "        translated=self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text with emoji  and symbol â™¡.\n",
      "['Example text with emoji ', '', 'and symbol', 'â™ª â™ª â™ª', \"- I 'm not .\"]\n"
     ]
    }
   ],
   "source": [
    "translator=Translator()\n",
    "translator_emojis = Translator_emojis()\n",
    "print(translator.translate(src_lan='spa_Latn',tgt_lan='eng_Latn',text=\"Example text with emoji ðŸ¤¯ and symbol â™¡.\"))\n",
    "print(translator_emojis.translate(src_lan='spa_Latn',tgt_lan='eng_Latn',text=\"Example text with emoji ðŸ¤¯ and symbol â™¡.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergin Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_csv('Dataset_tab.csv',sep='\\t')\n",
    "\n",
    "english = pd.read_csv('train.csv',sep='\\t')\n",
    "english['toxic_sentence_translated']=np.array(english['en_toxic_comment'])\n",
    "english['neutral_sentence_translated']=np.array(english['en_neutral_comment'])\n",
    "lang = [\"eng_Latn\" for i in range(len(english))]\n",
    "english['original_language']=lang\n",
    "english = english.rename(columns={'en_toxic_comment': 'toxic_sentence_not_translated', 'en_neutral_comment': 'neutral_sentence_not_translated'})\n",
    "english.to_csv('english_lines_translated.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = pd.concat([languages,english])\n",
    "total_dataset.to_csv('total_dataset_translated.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages = pd.read_csv('total_dataset_translated.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_sentence_not_translated</th>\n",
       "      <th>neutral_sentence_not_translated</th>\n",
       "      <th>toxic_sentence_translated</th>\n",
       "      <th>neutral_sentence_translated</th>\n",
       "      <th>original_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>then all of a sudden i see her , shes now got ...</td>\n",
       "      <td>All of a sudden i see her, she is all grown up.</td>\n",
       "      <td>then all of a sudden i see her , shes now got ...</td>\n",
       "      <td>All of a sudden i see her, she is all grown up.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My page should be protected first so that wort...</td>\n",
       "      <td>My page should be protected first so that unpl...</td>\n",
       "      <td>My page should be protected first so that wort...</td>\n",
       "      <td>My page should be protected first so that unpl...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You made a mistake you ass.</td>\n",
       "      <td>You made a mistake.</td>\n",
       "      <td>You made a mistake you ass.</td>\n",
       "      <td>You made a mistake.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you know more than these idiots , stay the cou...</td>\n",
       "      <td>you know more than these people , stay the cou...</td>\n",
       "      <td>you know more than these idiots , stay the cou...</td>\n",
       "      <td>you know more than these people , stay the cou...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>piss me off , fuckin jerk , get on my nerves .</td>\n",
       "      <td>get on my nerves</td>\n",
       "      <td>piss me off , fuckin jerk , get on my nerves .</td>\n",
       "      <td>get on my nerves</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23339</th>\n",
       "      <td>when they do shit like this .</td>\n",
       "      <td>when they do stuff like this</td>\n",
       "      <td>when they do shit like this .</td>\n",
       "      <td>when they do stuff like this</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23340</th>\n",
       "      <td>but if saying \" fuck that group \" is much more...</td>\n",
       "      <td>but if saying\" that group is bad\" is much more...</td>\n",
       "      <td>but if saying \" fuck that group \" is much more...</td>\n",
       "      <td>but if saying\" that group is bad\" is much more...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23341</th>\n",
       "      <td>it hurts how judgemental assholes view them , ...</td>\n",
       "      <td>It hurts how judgemental that people view them...</td>\n",
       "      <td>it hurts how judgemental assholes view them , ...</td>\n",
       "      <td>It hurts how judgemental that people view them...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23342</th>\n",
       "      <td>shit we probably literally blow that up in a w...</td>\n",
       "      <td>We probably litteralky blow that up in a week.</td>\n",
       "      <td>shit we probably literally blow that up in a w...</td>\n",
       "      <td>We probably litteralky blow that up in a week.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23343</th>\n",
       "      <td>if anyone deserved it , it was this shit bag .</td>\n",
       "      <td>if anyone deserved it , it was this bad bag .</td>\n",
       "      <td>if anyone deserved it , it was this shit bag .</td>\n",
       "      <td>if anyone deserved it , it was this bad bag .</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23344 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           toxic_sentence_not_translated  \\\n",
       "0      then all of a sudden i see her , shes now got ...   \n",
       "1      My page should be protected first so that wort...   \n",
       "2                            You made a mistake you ass.   \n",
       "3      you know more than these idiots , stay the cou...   \n",
       "4         piss me off , fuckin jerk , get on my nerves .   \n",
       "...                                                  ...   \n",
       "23339                      when they do shit like this .   \n",
       "23340  but if saying \" fuck that group \" is much more...   \n",
       "23341  it hurts how judgemental assholes view them , ...   \n",
       "23342  shit we probably literally blow that up in a w...   \n",
       "23343     if anyone deserved it , it was this shit bag .   \n",
       "\n",
       "                         neutral_sentence_not_translated  \\\n",
       "0        All of a sudden i see her, she is all grown up.   \n",
       "1      My page should be protected first so that unpl...   \n",
       "2                                    You made a mistake.   \n",
       "3      you know more than these people , stay the cou...   \n",
       "4                                       get on my nerves   \n",
       "...                                                  ...   \n",
       "23339                       when they do stuff like this   \n",
       "23340  but if saying\" that group is bad\" is much more...   \n",
       "23341  It hurts how judgemental that people view them...   \n",
       "23342     We probably litteralky blow that up in a week.   \n",
       "23343      if anyone deserved it , it was this bad bag .   \n",
       "\n",
       "                               toxic_sentence_translated  \\\n",
       "0      then all of a sudden i see her , shes now got ...   \n",
       "1      My page should be protected first so that wort...   \n",
       "2                            You made a mistake you ass.   \n",
       "3      you know more than these idiots , stay the cou...   \n",
       "4         piss me off , fuckin jerk , get on my nerves .   \n",
       "...                                                  ...   \n",
       "23339                      when they do shit like this .   \n",
       "23340  but if saying \" fuck that group \" is much more...   \n",
       "23341  it hurts how judgemental assholes view them , ...   \n",
       "23342  shit we probably literally blow that up in a w...   \n",
       "23343     if anyone deserved it , it was this shit bag .   \n",
       "\n",
       "                             neutral_sentence_translated original_language  \n",
       "0        All of a sudden i see her, she is all grown up.          eng_Latn  \n",
       "1      My page should be protected first so that unpl...          eng_Latn  \n",
       "2                                    You made a mistake.          eng_Latn  \n",
       "3      you know more than these people , stay the cou...          eng_Latn  \n",
       "4                                       get on my nerves          eng_Latn  \n",
       "...                                                  ...               ...  \n",
       "23339                       when they do stuff like this          eng_Latn  \n",
       "23340  but if saying\" that group is bad\" is much more...          eng_Latn  \n",
       "23341  It hurts how judgemental that people view them...          eng_Latn  \n",
       "23342     We probably litteralky blow that up in a week.          eng_Latn  \n",
       "23343      if anyone deserved it , it was this bad bag .          eng_Latn  \n",
       "\n",
       "[23344 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.array(all_languages)\n",
    "X_train,X_test,y_train,y_test = train_test_split(array[:,2],array[:,3],train_size=0.8,stratify=array[:,-1]) #Estratificado por idioma para manter la distribuciÃ³n de clases en ambos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        super().__init__()\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "train_dataset = dataset(X_train,y_train)\n",
    "test_dataset = dataset(X_test,y_test)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(next(iter(train_dataloader)))\n",
    "b=a[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('s-nlp/bart-base-detox')\n",
    "model = BartForConditionalGeneration.from_pretrained('s-nlp/bart-base-detox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(b, return_tensors=\"pt\", padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1594,   939,   205, 13512,   118,  2463,  2156,    47,   342,\n",
       "         42167, 22396, 17487,     2, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  7877, 27726,    82,   939,    33,   655,  1145,   479,     2,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   118,   224,  8382,  2662,    15,    84,    53,  1872,     8,\n",
       "           109,  1085,   479,     2, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  1075, 41460,   127, 23523,  4709,  1090, 11616,  1149, 27398,\n",
       "            95,  1224,   160,     5, 23523,  2888,     2, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 10219,    45,    28, 12103,    59,    42,   479,     2, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  3226,  1009,    14,   128,    29,   141,     5,   481,   210,\n",
       "          1364,   111, 14063,    14, 26536,    81,    49,  1321,   479,     2,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   757,  2379, 26284,     9, 20125,    16,    10,   269, 16881,\n",
       "          1114,   479,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 25407,   449, 33594,    74,  4410,  1115,   167, 39984,   114,\n",
       "            51,  1381,    14, 15328,   479,     2, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   428,  3239,   146,   162, 27785,   939,   109,    99, 17487,\n",
       "           939,   128,   119, 10985,   111, 18134,   111,     2, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  9713,   117,  5170,    51,  4021,    41,  8978,    23,   110,\n",
       "          8446,    23,   385,   705,  2156,   741, 27825,  5378,  2990,  1344,\n",
       "             2, 50265, 50265],\n",
       "        [    0, 24258,   108,   104,  5531,  8856, 24394,  5102,  3755,  2118,\n",
       "         11974,  1941, 41222,  2076,  4248,   274, 40230, 11595,  5289, 10751,\n",
       "           104, 14279,     2],\n",
       "        [    0,  1594,    52,   222,  2156,    52,  1979,   128,    90,    28,\n",
       "           519,   209, 23523,  1272,   479,     2, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 22371,   127,    78, 33411,    13,  3940,  4053,  2156,    10,\n",
       "          2544,    42,  9953,    10, 32594, 29784,     2, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  6025, 15328,   197,    28,   136,     5,   488, 27785,     2,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 43258,   939,  2813,   939,    21,   164,     7, 12138, 16963,\n",
       "             2, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 29510,    52,   109,    98, 17811,  2156,    42, 15328,    40,\n",
       "           535,   479,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  7608,   197,    10, 20397,   697,    19,   215,    10,   631,\n",
       "           116,  4511,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 25800,     5, 26536,   473,    24,  6565,    10,  2051,     7,\n",
       "          3993,  5506,    82, 17487,     2, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   118,   115,   213,   465,  2682,     7,   109,    53,   939,\n",
       "           128,   119, 22414,    25, 15328,     2, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   417,  2533,   486,   162,    10,  3605,  5406,    47, 42167,\n",
       "           479,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  2716,  2577,     5,   869,  3568,   106,    95, 11620,   841,\n",
       "             7,  3211,    55, 15328,   479,     2, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 43258,   101,    14,   460,  2594,    23,   182,   593,  1186,\n",
       "           479,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   118,   129,   856, 19667,    19,    10,   205,  1812,  7606,\n",
       "             9,   106,   479,     2, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   627, 15328,    69,   308,   247,   473,    16,    95,    25,\n",
       "          1099,  2156,   114,    45,  3007,   479,     2, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  1708,    32,    47,  4225,    50, 39984,   116,  1648,   600,\n",
       "            24,    16,    11, 14840,    24,   161, 14185,  1809,   328,     2,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  8155,  2029,    10, 15328,   114,    51,   128,   241, 10789,\n",
       "             8,   239, 17487,     2, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   700,    26,    47,    58,  4003,  3314,    10, 38594,   479,\n",
       "             2, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   118,  5673,    14, 32594, 36831,  6612,  5996,     2, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,  1322,    47,  3127,    14, 14628, 10003,  2156,   181,  5906,\n",
       "         17487,   479,     2, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0, 12350,  6620,     7,     5,  7159,  9284,     4,   125,  3859,\n",
       "            16,   277,   183,     4,     2, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   438,  8015,   281,    64,   128,    90,   192, 15328,    77,\n",
       "            51,    32,  2913,    11,  4603,   479,     2, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265],\n",
       "        [    0,   271,  4832,   137,   442,    10,  2671, 17275,     9,  2512,\n",
       "           213,  1649,     5,  5135,   939,  4418,   479,     2, 50265, 50265,\n",
       "         50265, 50265, 50265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  1594,   939,   205, 13512,   118,  2463,  2156,    47,   342,\n",
       "        42167, 22396, 17487,     2, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "        50265, 50265, 50265])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaime/Escritorio/AIR/entornos/NLP/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "c=model.generate(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('omfg my fucking arsehole stepdad just turned off the fucking internet')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>omfg my stepdad just turned off the internet</s><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(c[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detox():\n",
    "    def __init__(self):\n",
    "        self.base_model_name='facebook/bart-base'\n",
    "        self.model_name = 's-nlp/bart-base-detox'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
    "\n",
    "    def detox(self,input):\n",
    "        input_ids = self.tokenizer.encode(input, return_tensors='pt')\n",
    "        output_ids = self.model.generate(input_ids)\n",
    "        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return output_text\n",
    "    \n",
    "    def train(self, dataloader):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

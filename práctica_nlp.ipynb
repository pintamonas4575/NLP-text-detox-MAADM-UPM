{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 15:35:28.794516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732113328.813651   11899 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732113328.819701   11899 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 15:35:28.842967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import regex as re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from googletrans import Translator\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available()  else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GRUPO A**\n",
    "* JAIME √ÅLVAREZ URUE√ëA\n",
    "* ALEJANDRO MENDOZA MEDINA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicaci√≥n de la pr√°ctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El reto sobre el que se va a trabajar en esta pr√°ctica es la tarea de detoxificaci√≥n de frases en distintos idiomas. <br>\n",
    "Como las oraciones pueden estar hasta en 9 idiomas distintos, la aproximaci√≥n elegida ser√° primero traducir todas las oraciones a un idioma (ingl√©s),\n",
    "y entrenar el detoxificador √∫nicamente en ingl√©s. <br>\n",
    "El pipeline de inferencia ser√° por tanto traducir la frase del idioma original al ingl√©s, detoxificar la frase con el detoxificador entrenado, y volver a traducir\n",
    "la frase al idioma original.\n",
    "Con respecto a los Datasets utilizados, se han utilizado 2. El dataset propio de la tarea que cuenta con 400 frases por cada idioma (9), y tambi√©n se ha utilizado un dataset en el que √∫nicamente tiene ejemplos en ingl√©s. <br>\n",
    "El modelo de NLP usado es el modelo T5 flan-base. Se ha modificado sus settings para que la tarea que desempe√±e sea detoxificar.<br>\n",
    "\n",
    "Para traducir se han explorado 3 aproximaciones:\n",
    "- Modelo NLLB-200-distilled-600M, que tiene todos los idiomas de la tarea. No obstante, no tokeniza todos los emojis <br>\n",
    "- NLLB-200-distilled-600M separando las oraciones por emojis, traduciendo cada porci√≥n de la frase original y volviendo a unir por emojis <br>\n",
    "- Google translate, que contiene todos los idiomas necesarios y s√≠ acepta todos los emojis <br>\n",
    "Se han implementado las 3 aproximaciones, y la que mejor resultados ofrece es Google Translate, por eso se elige finalmente ese m√©todo. <br>\n",
    "<br>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate all phrases (ALREADY DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate with Google Translate (accepts emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'eng_Latn': 'data/en-00000-of-00001.parquet', \n",
    "          'rus_Cyrl': 'data/ru-00000-of-00001.parquet', \n",
    "          'ukr_Cyrl': 'data/uk-00000-of-00001.parquet', \n",
    "          'deu_Latn': 'data/de-00000-of-00001.parquet', \n",
    "          'spa_Latn': 'data/es-00000-of-00001.parquet', \n",
    "          'amh_Ethi': 'data/am-00000-of-00001.parquet', \n",
    "          'zho_Hans': 'data/zh-00000-of-00001.parquet', \n",
    "          'arb_Arab': 'data/ar-00000-of-00001.parquet', \n",
    "          'hin_Deva': 'data/hi-00000-of-00001.parquet'}\n",
    "translator = Translator()\n",
    "all_data=[]\n",
    "for key, value in splits.items():\n",
    "    x=0\n",
    "    final_dataframe = pd.DataFrame(columns=['toxic_sentence_not_translated','neutral_sentence_not_translated','toxic_sentence_translated','neutral_sentence_translated','original_language'])\n",
    "    df = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + value)\n",
    "    array = np.array(df)\n",
    "    for instance in array:\n",
    "        input=[]\n",
    "        input.append(instance[0])\n",
    "        input.append(instance[1])\n",
    "        toxic_translated = translator.translate(instance[0]).text\n",
    "        input.append(toxic_translated)\n",
    "        non_toxic_translated = translator.translate(instance[1]).text\n",
    "        input.append(non_toxic_translated)\n",
    "        input.append(key)\n",
    "        final_dataframe.loc[x]=input\n",
    "        x+=1\n",
    "    all_data.append(final_dataframe)\n",
    "big_dataframe=pd.concat(all_data)\n",
    "big_dataframe.to_csv('Dataset_tab.csv',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate with NLLB-200-distilled-600M (does not accept all emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con emojis\n",
    "class Translator_emojis():\n",
    "    def __init__(self):\n",
    "        # Use the original NLLB tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "    def translate(self, src_lan, tgt_lan, text):\n",
    "        # Split text into words and emojis to handle emojis separately\n",
    "        words = re.split(r'(\\p{So}|\\p{Cn})', text, flags=re.UNICODE)\n",
    "        translated_text = []\n",
    "        for word in words:\n",
    "            if word and not re.search(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF]', word):\n",
    "                # Only translate text segments, skip emojis\n",
    "                text_with_lang_code = f\"{src_lan} {word}\"\n",
    "                inputs = self.tokenizer(text_with_lang_code, return_tensors=\"pt\")\n",
    "                translated_tokens = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lan)\n",
    "                )\n",
    "                translated_word = self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                # If it's an emoji, keep it as is\n",
    "                print(u'word')\n",
    "                translated_text += u'\\U0001f604'\n",
    "\n",
    "            # Append translated word or emoji to result\n",
    "            translated_text.append(translated_word)\n",
    "        \n",
    "        return translated_text\n",
    "    \n",
    "#Omitiendo emojis\n",
    "class Translator():\n",
    "    def __init__(self):\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        self.model=AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "    def translate(self,src_lan,tgt_lan,text):\n",
    "        text=f\"{src_lan} {text}\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        translated_tokens = self.model.generate(** inputs, forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lan))\n",
    "        translated=self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text with emoji  and symbol ‚ô°.\n",
      "['Example text with emoji ', '', 'and symbol', '‚ô™ ‚ô™ ‚ô™', \"- I 'm not .\"]\n"
     ]
    }
   ],
   "source": [
    "translator=Translator()\n",
    "translator_emojis = Translator_emojis()\n",
    "print(translator.translate(src_lan='spa_Latn',tgt_lan='eng_Latn',text=\"Example text with emoji ü§Ø and symbol ‚ô°.\"))\n",
    "print(translator_emojis.translate(src_lan='spa_Latn',tgt_lan='eng_Latn',text=\"Example text with emoji ü§Ø and symbol ‚ô°.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergin Datasets (ALREADY DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_csv('Dataset_tab.csv',sep='\\t')\n",
    "\n",
    "english = pd.read_csv('train.csv',sep='\\t')\n",
    "english['toxic_sentence_translated']=np.array(english['en_toxic_comment'])\n",
    "english['neutral_sentence_translated']=np.array(english['en_neutral_comment'])\n",
    "lang = [\"eng_Latn\" for i in range(len(english))]\n",
    "english['original_language']=lang\n",
    "english = english.rename(columns={'en_toxic_comment': 'toxic_sentence_not_translated', 'en_neutral_comment': 'neutral_sentence_not_translated'})\n",
    "english.to_csv('english_lines_translated.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = pd.concat([languages,english])\n",
    "total_dataset.to_csv('total_dataset_translated.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,inputs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    inputs = \" \".join(inputs.split())\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids=input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "def compute_rouge(hypothesis,reference):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return scores\n",
    "\n",
    "def detox(model,input,language):\n",
    "    language_mapping = {\n",
    "        'eng_Latn': 'en',  # English\n",
    "        'rus_Cyrl': 'ru',  # Russian\n",
    "        'ukr_Cyrl': 'uk',  # Ukrainian\n",
    "        'deu_Latn': 'de',  # German\n",
    "        'spa_Latn': 'es',  # Spanish\n",
    "        'amh_Ethi': 'am',  # Amharic\n",
    "        'zho_Hans': 'zh-cn',  # Simplified Chinese\n",
    "        'arb_Arab': 'ar',  # Arabic\n",
    "        'hin_Deva': 'hi'   # Hindi\n",
    "    }\n",
    "    language_short=language_mapping[language]\n",
    "    translator=Translator()\n",
    "    translated=translator.translate(input,src=language_short,dest='en').text\n",
    "    detox=generate(model,translated)\n",
    "    original_language_detox = translator.translate(detox,src='en',dest=language_short).text\n",
    "    return original_language_detox\n",
    "\n",
    "def detox_multi_return(model, input: str, language: str):\n",
    "    language_mapping = {\n",
    "        'eng_Latn': 'en',  # English\n",
    "        'rus_Cyrl': 'ru',  # Russian\n",
    "        'ukr_Cyrl': 'uk',  # Ukrainian\n",
    "        'deu_Latn': 'de',  # German\n",
    "        'spa_Latn': 'es',  # Spanish\n",
    "        'amh_Ethi': 'am',  # Amharic\n",
    "        'zho_Hans': 'zh-cn',  # Simplified Chinese\n",
    "        'arb_Arab': 'ar',  # Arabic\n",
    "        'hin_Deva': 'hi'   # Hindi\n",
    "    }\n",
    "    language_short=language_mapping[language]\n",
    "    translator=Translator()\n",
    "    translated=translator.translate(input,src=language_short,dest='en').text\n",
    "    detox=generate(model,translated)\n",
    "    original_language_detox = translator.translate(detox,src='en',dest=language_short).text\n",
    "    return (translated, detox, original_language_detox)\n",
    "\n",
    "def calculate_mean(data, rouge_key):\n",
    "    values = np.array([[entry[rouge_key]['r'], entry[rouge_key]['p'], entry[rouge_key]['f']] for entry in data])\n",
    "    return values.mean(axis=0)\n",
    "\n",
    "def loss_fn(outputs, targets,mask):\n",
    "    a=nn.CrossEntropyLoss(reduction='none')(outputs,targets)\n",
    "    mask=mask.float()\n",
    "    b=torch.tensordot(a,mask)\n",
    "    return b/torch.sum(mask)\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between two texts.\n",
    "    \"\"\"\n",
    "    set1 = set(text1.split())  # Tokenize into words\n",
    "    set2 = set(text2.split())\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages = pd.read_csv('total_dataset_translated.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_sentence_not_translated</th>\n",
       "      <th>neutral_sentence_not_translated</th>\n",
       "      <th>toxic_sentence_translated</th>\n",
       "      <th>neutral_sentence_translated</th>\n",
       "      <th>original_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>then all of a sudden i see her , shes now got ...</td>\n",
       "      <td>All of a sudden i see her, she is all grown up.</td>\n",
       "      <td>then all of a sudden i see her , shes now got ...</td>\n",
       "      <td>All of a sudden i see her, she is all grown up.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My page should be protected first so that wort...</td>\n",
       "      <td>My page should be protected first so that unpl...</td>\n",
       "      <td>My page should be protected first so that wort...</td>\n",
       "      <td>My page should be protected first so that unpl...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You made a mistake you ass.</td>\n",
       "      <td>You made a mistake.</td>\n",
       "      <td>You made a mistake you ass.</td>\n",
       "      <td>You made a mistake.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you know more than these idiots , stay the cou...</td>\n",
       "      <td>you know more than these people , stay the cou...</td>\n",
       "      <td>you know more than these idiots , stay the cou...</td>\n",
       "      <td>you know more than these people , stay the cou...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>piss me off , fuckin jerk , get on my nerves .</td>\n",
       "      <td>get on my nerves</td>\n",
       "      <td>piss me off , fuckin jerk , get on my nerves .</td>\n",
       "      <td>get on my nerves</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23339</th>\n",
       "      <td>when they do shit like this .</td>\n",
       "      <td>when they do stuff like this</td>\n",
       "      <td>when they do shit like this .</td>\n",
       "      <td>when they do stuff like this</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23340</th>\n",
       "      <td>but if saying \" fuck that group \" is much more...</td>\n",
       "      <td>but if saying\" that group is bad\" is much more...</td>\n",
       "      <td>but if saying \" fuck that group \" is much more...</td>\n",
       "      <td>but if saying\" that group is bad\" is much more...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23341</th>\n",
       "      <td>it hurts how judgemental assholes view them , ...</td>\n",
       "      <td>It hurts how judgemental that people view them...</td>\n",
       "      <td>it hurts how judgemental assholes view them , ...</td>\n",
       "      <td>It hurts how judgemental that people view them...</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23342</th>\n",
       "      <td>shit we probably literally blow that up in a w...</td>\n",
       "      <td>We probably litteralky blow that up in a week.</td>\n",
       "      <td>shit we probably literally blow that up in a w...</td>\n",
       "      <td>We probably litteralky blow that up in a week.</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23343</th>\n",
       "      <td>if anyone deserved it , it was this shit bag .</td>\n",
       "      <td>if anyone deserved it , it was this bad bag .</td>\n",
       "      <td>if anyone deserved it , it was this shit bag .</td>\n",
       "      <td>if anyone deserved it , it was this bad bag .</td>\n",
       "      <td>eng_Latn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23344 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           toxic_sentence_not_translated  \\\n",
       "0      then all of a sudden i see her , shes now got ...   \n",
       "1      My page should be protected first so that wort...   \n",
       "2                            You made a mistake you ass.   \n",
       "3      you know more than these idiots , stay the cou...   \n",
       "4         piss me off , fuckin jerk , get on my nerves .   \n",
       "...                                                  ...   \n",
       "23339                      when they do shit like this .   \n",
       "23340  but if saying \" fuck that group \" is much more...   \n",
       "23341  it hurts how judgemental assholes view them , ...   \n",
       "23342  shit we probably literally blow that up in a w...   \n",
       "23343     if anyone deserved it , it was this shit bag .   \n",
       "\n",
       "                         neutral_sentence_not_translated  \\\n",
       "0        All of a sudden i see her, she is all grown up.   \n",
       "1      My page should be protected first so that unpl...   \n",
       "2                                    You made a mistake.   \n",
       "3      you know more than these people , stay the cou...   \n",
       "4                                       get on my nerves   \n",
       "...                                                  ...   \n",
       "23339                       when they do stuff like this   \n",
       "23340  but if saying\" that group is bad\" is much more...   \n",
       "23341  It hurts how judgemental that people view them...   \n",
       "23342     We probably litteralky blow that up in a week.   \n",
       "23343      if anyone deserved it , it was this bad bag .   \n",
       "\n",
       "                               toxic_sentence_translated  \\\n",
       "0      then all of a sudden i see her , shes now got ...   \n",
       "1      My page should be protected first so that wort...   \n",
       "2                            You made a mistake you ass.   \n",
       "3      you know more than these idiots , stay the cou...   \n",
       "4         piss me off , fuckin jerk , get on my nerves .   \n",
       "...                                                  ...   \n",
       "23339                      when they do shit like this .   \n",
       "23340  but if saying \" fuck that group \" is much more...   \n",
       "23341  it hurts how judgemental assholes view them , ...   \n",
       "23342  shit we probably literally blow that up in a w...   \n",
       "23343     if anyone deserved it , it was this shit bag .   \n",
       "\n",
       "                             neutral_sentence_translated original_language  \n",
       "0        All of a sudden i see her, she is all grown up.          eng_Latn  \n",
       "1      My page should be protected first so that unpl...          eng_Latn  \n",
       "2                                    You made a mistake.          eng_Latn  \n",
       "3      you know more than these people , stay the cou...          eng_Latn  \n",
       "4                                       get on my nerves          eng_Latn  \n",
       "...                                                  ...               ...  \n",
       "23339                       when they do stuff like this          eng_Latn  \n",
       "23340  but if saying\" that group is bad\" is much more...          eng_Latn  \n",
       "23341  It hurts how judgemental that people view them...          eng_Latn  \n",
       "23342     We probably litteralky blow that up in a week.          eng_Latn  \n",
       "23343      if anyone deserved it , it was this bad bag .          eng_Latn  \n",
       "\n",
       "[23344 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(all_languages['toxic_sentence_translated'],all_languages['neutral_sentence_translated'],train_size=0.8,random_state=2,stratify=all_languages['original_language']) #Estratificado por idioma para manter la distribuci√≥n de clases en ambos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,targets,max_len=50):\n",
    "        self.dataset = dataset\n",
    "        self.targets=targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        phrase = self.dataset.iloc[index]\n",
    "        target = self.targets.iloc[index]\n",
    "\n",
    "        phrase = \" \".join(phrase.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "                phrase,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                pad_to_max_length=True,\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True\n",
    "            )\n",
    "        target = \" \".join(target.split())\n",
    "        outputs = self.tokenizer.encode_plus(\n",
    "                target,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                pad_to_max_length=True,\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True\n",
    "            )\n",
    "        \n",
    "        encoder_ids = inputs['input_ids']\n",
    "        encoder_mask = inputs['attention_mask']\n",
    "        decoder_ids = outputs['input_ids'][:-1]\n",
    "        decoder_mask = outputs['attention_mask'][:-1]\n",
    "        decoder_outs = outputs['input_ids'][1:]\n",
    "        decoder_outs_mask = outputs['attention_mask'][1:]\n",
    "\n",
    "        return {\n",
    "            'encoder_ids': torch.tensor(encoder_ids, dtype=torch.long),\n",
    "            'encoder_mask': torch.tensor(encoder_mask, dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(decoder_ids, dtype=torch.long),\n",
    "            'decoder_input_mask': torch.tensor(decoder_mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(decoder_outs, dtype=torch.float),\n",
    "            'target_mask': torch.tensor(decoder_outs_mask, dtype=torch.float),\n",
    "            'no_tokenize_target':target\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(X_train,y_train)\n",
    "test_dataset = CustomDataset(X_test,y_test)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Tell the T5 model which task has to carry out\n",
    "model.config.task_specific_params={\n",
    "    \"neutralization\": {\n",
    "      \"early_stopping\": True,\n",
    "      \"length_penalty\": 2.0,\n",
    "      \"max_length\": 100,\n",
    "      \"min_length\": 50,\n",
    "      \"no_repeat_ngram_size\": 3,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"neutralize: \"\n",
    "    }\n",
    "}\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de par√°metros: 247,577,856\n",
      "Par√°metros entrenables: 247,577,856\n",
      "Par√°metros congelados: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total de par√°metros: {total_params:,}\")\n",
    "print(f\"Par√°metros entrenables: {trainable_params:,}\")\n",
    "print(f\"Par√°metros congelados: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,epochs,train_dataloader,test_dataloader):\n",
    "    checkpoint_path = \"t5_checkpoint.pth\"\n",
    "    for epoch in range(epochs):\n",
    "        losses=[]\n",
    "        print('-'*100)\n",
    "        print(f'Epoch number: {epoch}.')\n",
    "        model.train()\n",
    "        time1=time.time()\n",
    "        print('Training...')\n",
    "        for data in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            targets=data['targets'].to(device,dtype=torch.long)\n",
    "            target_mask=data['target_mask'].to(device,dtype=torch.long)\n",
    "            encoder_ids=data['encoder_ids'].to(device,dtype=torch.long)\n",
    "            encoder_mask=data['encoder_mask'].to(device,dtype=torch.long)\n",
    "            decoder_input_ids=data['decoder_input_ids'].to(device,dtype=torch.long)\n",
    "            decoder_input_mask=data['decoder_input_mask'].to(device,dtype=torch.long)\n",
    "            output=model.forward(encoder_ids,encoder_mask,decoder_input_ids,decoder_input_mask)\n",
    "            del encoder_ids,encoder_mask,decoder_input_ids,decoder_input_mask\n",
    "            output=output.logits\n",
    "            output=torch.transpose(output,2,1)\n",
    "            optimizer.zero_grad()\n",
    "            loss=loss_fn(output,targets,target_mask)\n",
    "            del targets,target_mask,output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        all_losses=[]\n",
    "        model.eval()\n",
    "        print('Testing...')\n",
    "        for test in tqdm(test_dataloader, desc=f\"Testing Epoch {epoch + 1}/{epochs}\"):\n",
    "            targets=test['targets'].to(device,dtype=torch.long)\n",
    "            target_mask=test['target_mask'].to(device,dtype=torch.long)\n",
    "            encoder_ids=test['encoder_ids'].to(device,dtype=torch.long)\n",
    "            encoder_mask=test['encoder_mask'].to(device,dtype=torch.long)\n",
    "            decoder_input_ids=test['decoder_input_ids'].to(device,dtype=torch.long)\n",
    "            decoder_input_mask=test['decoder_input_mask'].to(device,dtype=torch.long)\n",
    "            \n",
    "            output=model.forward(encoder_ids,encoder_mask,decoder_input_ids,decoder_input_mask)\n",
    "            \n",
    "            del encoder_ids,encoder_mask,decoder_input_ids,decoder_input_mask\n",
    "            output=output.logits\n",
    "            output=torch.transpose(output,2,1)\n",
    "            loss=loss_fn(output,targets,target_mask)\n",
    "            del output\n",
    "            all_losses.append(loss.item())\n",
    "            # phrases=test['no_tokenize_target']\n",
    "            # predicted_target=[generate(model,target) for target in phrases]\n",
    "            # out=compute_rouge(phrases,predicted_target)\n",
    "            # r_1 = calculate_mean(out, 'rouge-1')\n",
    "            # r_2 = calculate_mean(out, 'rouge-2')\n",
    "            # r_l = calculate_mean(out, 'rouge-l')\n",
    "            del targets,target_mask,\n",
    "        loss_epoch=np.array(all_losses).mean()\n",
    "        print(f'Loss in test set:{loss}.')\n",
    "        # print(f'Rouge-1: \\'r\\': {r_1[0]}, \\'p\\': {r_1[1]}, \\'f\\': {r_1[2]}')\n",
    "        # print(f'Rouge-2: \\'r\\': {r_2[0]}, \\'p\\': {r_2[1]}, \\'f\\': {r_2[2]}')\n",
    "        # print(f'Rouge-l: \\'r\\': {r_l[0]}, \\'p\\': {r_l[1]}, \\'f\\': {r_l[2]}')\n",
    "        losses.append(loss_epoch)\n",
    "        print(f'Time: {time.time()-time1}')\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": losses,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 0.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|          | 0/584 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:58<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:1.1723066568374634.\n",
      "Time: 196.3087465763092\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 1.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.9527485370635986.\n",
      "Time: 196.98666763305664\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 2.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.9848648905754089.\n",
      "Time: 197.03592491149902\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 3.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:1.0480660200119019.\n",
      "Time: 197.1304533481598\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 4.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:1.025219440460205.\n",
      "Time: 197.0929343700409\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 5.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.9371489882469177.\n",
      "Time: 197.140545129776\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 6.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:16<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:1.2057018280029297.\n",
      "Time: 196.6441102027893\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 7.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:17<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.8589909076690674.\n",
      "Time: 196.9320924282074\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 8.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:16<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.6574808359146118.\n",
      "Time: 196.76171827316284\n",
      "Checkpoint saved at t5_checkpoint.pth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch number: 9.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [02:59<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:16<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in test set:0.8399816155433655.\n",
      "Time: 196.5332260131836\n",
      "Checkpoint saved at t5_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,10,train_dataloader,test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"t5_big.pth\"\n",
    "# checkpoint_path = \"t5_checkpoint.pth\"\n",
    "checkpoint=torch.load(checkpoint_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available()  else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model:T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", device_map=device)\n",
    "\n",
    "# Tell the T5 model which task has to carry out\n",
    "model.config.task_specific_params={\n",
    "  \"neutralization\": {\n",
    "    \"early_stopping\": True,\n",
    "    \"length_penalty\": 2.0,\n",
    "    \"max_length\": 100,\n",
    "    \"min_length\": 50,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"num_beams\": 4,\n",
    "    \"prefix\": \"neutralize: \"\n",
    "  }\n",
    "}\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params =  model.parameters())\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on custom english phrases (detox model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_english_example(model: T5ForConditionalGeneration, toxic_sentence: str) -> None:\n",
    "    detox_sentence=generate(model,toxic_sentence)\n",
    "    out=compute_rouge(toxic_sentence,detox_sentence)\n",
    "    print(f'Toxic sentence: {toxic_sentence}')\n",
    "    print(f'Detox sentence: {detox_sentence}')\n",
    "    r_1 = calculate_mean(out, 'rouge-1')\n",
    "    r_2 = calculate_mean(out, 'rouge-2')\n",
    "    r_l = calculate_mean(out, 'rouge-l')\n",
    "    print(f'Rouge-1: \\'r\\': {r_1[0]:.2f}, \\'p\\': {r_1[1]:.3f}, \\'f\\': {r_1[2]:.3f}')\n",
    "    print(f'Rouge-2: \\'r\\': {r_2[0]:.2f}, \\'p\\': {r_2[1]:.3f}, \\'f\\': {r_2[2]:.3f}')\n",
    "    print(f'Rouge-l: \\'r\\': {r_l[0]:.2f}, \\'p\\': {r_l[1]:.3f}, \\'f\\': {r_l[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: Stupid people make things wrong. You are stupid\n",
      "Detox sentence: Stupid people make things wrong\n",
      "Rouge-1: 'r': 1.00, 'p': 0.625, 'f': 0.769\n",
      "Rouge-2: 'r': 1.00, 'p': 0.571, 'f': 0.727\n",
      "Rouge-l: 'r': 1.00, 'p': 0.625, 'f': 0.769\n"
     ]
    }
   ],
   "source": [
    "# Sentence with a mixture of a bad word well used (meaning) and that same word used as an insult.\n",
    "sentence='Stupid people make things wrong. You are stupid'\n",
    "detoxify_english_example(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: My bloody brother is a sore loser\n",
      "Detox sentence: My brother is a loser\n",
      "Rouge-1: 'r': 1.00, 'p': 0.714, 'f': 0.833\n",
      "Rouge-2: 'r': 0.50, 'p': 0.333, 'f': 0.400\n",
      "Rouge-l: 'r': 1.00, 'p': 0.714, 'f': 0.833\n"
     ]
    }
   ],
   "source": [
    "# Sentence with a swear and a characterisc of a person\n",
    "sentence='My bloody brother is a sore loser'\n",
    "detoxify_english_example(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: That very person has such a lovely soul\n",
      "Detox sentence: That very person has such a lovely soul\n",
      "Rouge-1: 'r': 1.00, 'p': 1.000, 'f': 1.000\n",
      "Rouge-2: 'r': 1.00, 'p': 1.000, 'f': 1.000\n",
      "Rouge-l: 'r': 1.00, 'p': 1.000, 'f': 1.000\n"
     ]
    }
   ],
   "source": [
    "# Non-toxic sentence\n",
    "sentence='That very person has such a lovely soul'\n",
    "detoxify_english_example(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: I hate you, you are such a dick head, i really don¬¥t want to play you anymore because you are a completely asshole\n",
      "Detox sentence: I hate you, i really don t want to play you anymore because you are completely unsuitable\n",
      "Rouge-1: 'r': 0.81, 'p': 0.684, 'f': 0.743\n",
      "Rouge-2: 'r': 0.62, 'p': 0.476, 'f': 0.541\n",
      "Rouge-l: 'r': 0.81, 'p': 0.684, 'f': 0.743\n"
     ]
    }
   ],
   "source": [
    "# Full toxic sentence\n",
    "sentence='I hate you, you are such a dick head, i really don¬¥t want to play you anymore because you are a completely asshole'\n",
    "detoxify_english_example(model, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing toxic sentences in different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_multilanguage_example(model: T5ForConditionalGeneration, toxic_sentence: str, language: str) -> None:\n",
    "    toxic_translated, detox_sentence, detox_detranslated=detox_multi_return(model,sentence,language)\n",
    "    out=compute_rouge(sentence,detox_detranslated)\n",
    "    print(f'Toxic sentence: {toxic_sentence}')\n",
    "    print(f'Toxic translated sentence: {toxic_translated}')\n",
    "    print(f'Detox sentence: {detox_sentence}')\n",
    "    print(f'Detox detranslated sentence: {detox_detranslated}')\n",
    "    r_1 = calculate_mean(out, 'rouge-1')\n",
    "    r_2 = calculate_mean(out, 'rouge-2')\n",
    "    r_l = calculate_mean(out, 'rouge-l')\n",
    "    print(f'Rouge-1: \\'r\\': {r_1[0]:.2f}, \\'p\\': {r_1[1]:.3f}, \\'f\\': {r_1[2]:.3f}')\n",
    "    print(f'Rouge-2: \\'r\\': {r_2[0]:.2f}, \\'p\\': {r_2[1]:.3f}, \\'f\\': {r_2[2]:.3f}')\n",
    "    print(f'Rouge-l: \\'r\\': {r_l[0]:.2f}, \\'p\\': {r_l[1]:.3f}, \\'f\\': {r_l[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: Eres muy tonto, que te crees listo y eres estupido.\n",
      "Toxic translated sentence: You are very stupid, you think you are smart and you are stupid.\n",
      "Detox sentence: You think you are smart and you are not smart.\n",
      "Detox detranslated sentence: Crees que eres inteligente y no lo eres.\n",
      "Rouge-1: 'r': 0.43, 'p': 0.300, 'f': 0.353\n",
      "Rouge-2: 'r': 0.00, 'p': 0.000, 'f': 0.000\n",
      "Rouge-l: 'r': 0.43, 'p': 0.300, 'f': 0.353\n"
     ]
    }
   ],
   "source": [
    "sentence='Eres muy tonto, que te crees listo y eres estupido.'\n",
    "language='spa_Latn'\n",
    "detoxify_multilanguage_example(model, sentence, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: Du bist ein komplettes Arschloch, du bist ein beschissener Arschlecker.\n",
      "Toxic translated sentence: You're a complete asshole, you're a fucking asslicker.\n",
      "Detox sentence: You're a complete idiot, you're a bad guy.\n",
      "Detox detranslated sentence: Du bist ein Vollidiot, du bist ein B√∂sewicht.\n",
      "Rouge-1: 'r': 0.67, 'p': 0.500, 'f': 0.571\n",
      "Rouge-2: 'r': 0.50, 'p': 0.375, 'f': 0.429\n",
      "Rouge-l: 'r': 0.67, 'p': 0.500, 'f': 0.571\n"
     ]
    }
   ],
   "source": [
    "sentence='Du bist ein komplettes Arschloch, du bist ein beschissener Arschlecker.'\n",
    "language='deu_Latn'\n",
    "detoxify_multilanguage_example(model, sentence, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Jaccard-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic sentence: Eres una persona muy est√∫pida\n",
      "Detox sentence: eres una muy mala persona\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='Eres una persona muy est√∫pida'\n",
    "language='spa_Latn'\n",
    "detox_sentence=detox(model,sentence,language)\n",
    "print(f'Toxic sentence: {sentence}')\n",
    "print(f'Detox sentence: {detox_sentence}')\n",
    "jaccard_similarity(sentence,detox_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language eng_Latn: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in eng_Latn: 0.6654769663222726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language rus_Cyrl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:09<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in rus_Cyrl: 0.22380855807988914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language ukr_Cyrl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:03<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in ukr_Cyrl: 0.5393038658701307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language deu_Latn: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:56<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in deu_Latn: 0.25074488777608595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language spa_Latn: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:36<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in spa_Latn: 0.43945185445335444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language amh_Ethi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:56<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in amh_Ethi: 0.3460293323592472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language zho_Hans: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:10<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in zho_Hans: 0.1341261569786186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language arb_Arab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:50<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in arb_Arab: 0.03924499549187455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on language hin_Deva: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [02:07<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-Score in hin_Deva: 0.0055321336735810415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "languages=[\n",
    "    'eng_Latn',\n",
    "    'rus_Cyrl',\n",
    "    'ukr_Cyrl',\n",
    "    'deu_Latn',\n",
    "    'spa_Latn',\n",
    "    'amh_Ethi',\n",
    "    'zho_Hans',\n",
    "    'arb_Arab',\n",
    "    'hin_Deva'\n",
    "]\n",
    "all_similarities=[]\n",
    "for language in languages:\n",
    "    similarity=[]\n",
    "    dataframe=all_languages.iloc[np.squeeze(np.argwhere(np.array(all_languages['original_language'])=='spa_Latn'),axis=-1)]\n",
    "    _,X_test,_,y_test = train_test_split(dataframe['toxic_sentence_not_translated'],dataframe['neutral_sentence_not_translated'],train_size=0.8,random_state=2) #Estratificado por idioma para manter la distribuci√≥n de clases en ambos conjuntos\n",
    "    for sample in tqdm(X_test,desc=f\"Testing on language {language}\"):\n",
    "        detox_sentence=detox(model,sample,language)\n",
    "        sim= jaccard_similarity(sample,detox_sentence)\n",
    "        similarity.append(sim)\n",
    "    similarity=np.array(similarity)\n",
    "    all_similarities.append(np.mean(similarity))\n",
    "    print(f'J-Score in {language}: {np.mean(similarity)}')\n",
    "all_similarities=np.array(all_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean J-Score: 0.2937465278894505\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean J-Score: {np.mean(all_similarities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de Resultados con otros presentados en la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©trica usada es J-Score. Se han obtenido los siguientes resultados:\n",
    "- Ingl√©s: J-Score: 0.6654 --> Top 1\n",
    "- Ruso: J-Score: 0.223 --> Top 30\n",
    "- Ucraniano J-Score: 0.539 --> Top 11\n",
    "- Alem√°n J-Score: 0.2507 ---> Top 29\n",
    "- Espa√±ol: J-Score: 0.439 ---> Top 11\n",
    "- Am√°rico. J-Score: 0.346 ---> Top 3\n",
    "- Chino. J-Score: 0.134 ---> Top 13\n",
    "- √Årabe. J-Score: 0.039 ---> Top 35\n",
    "- Hind√∫. J-Score: 0.0055 ---> Top 35\n",
    "- J-Score medio de todos los idiomas: 0.293 ---> Top 25\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoclon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
